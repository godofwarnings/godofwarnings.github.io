---
layout: post
title: "HEP-JEPA: Building Foundation Models for Collider Physics"
description: "A quick dive into HEP-JEPA — our approach to learning abstract jet representations using self-supervised JEPA training."
categories: [Machine-Learning, High-Energy-Physics]
tags: [HEP, Foundation-Models, JEPA, Self-Supervised-Learning]
redirect_from:
  - /2025/05/29/
---

> Recently Presented - ICLR 2025 Workshop on HEP-JEPA

* Table of Contents
{:toc .toc}

# What is HEP-JEPA?

HEP-JEPA is a self-supervised foundation model designed to work with data from high-energy particle collisions — like those at the Large Hadron Collider (LHC). Built using the Joint Embedding Predictive Architecture (JEPA), it learns general-purpose representations of particle jets from raw, unlabeled data.

The idea is similar to how large language models learn "semantics" of text — except here, we’re learning the "semantics" of particle physics.

# Why Foundation Models in HEP!?

Traditional models in high-energy physics are trained task-by-task, often relying on hand-labeled simulation data. This isn't scalable, especially as LHC data grows into the hundreds of petabytes. HEP-JEPA aims to change that by:

- Pretraining once on large jet datasets (we use JetClass with 100M samples)
- Fine-tuning quickly for specific tasks
- Reducing compute costs and increasing reusability

# What Makes HEP-JEPA Different?

Most existing self-supervised methods in HEP follow either contrastive learning or generative (masked) approaches. But both have limitations:

> Contrastive learning depends heavily on the quality of negative samples  
> Masked modeling often focuses too much on local reconstruction

HEP-JEPA avoids both by predicting embeddings directly in latent space — no decoder needed. This makes training efficient and the representations more abstract and transferable.

# Architecture Overview

The model consists of:
- A **context encoder** that processes masked "visible" tokens (particles in a jet)
- A **target encoder** (an EMA version of the context encoder)
- A **predictor** that learns to predict target embeddings from the context

We tokenize particle jets using both kinematic and geometric features, apply spatial sampling strategies, and construct context/target blocks using custom sequencers.

# How Well Does It Work?

We benchmarked HEP-JEPA on:
1. **Jet classification** using JetClass  
2. **Top quark tagging** using the TQTR dataset  
3. **Regression tasks** for predicting jet observables like EFPs and jet shapes

The results? HEP-JEPA performs on par or better than state-of-the-art models like OmniJet-α and MPM — especially in few-shot learning settings.

Example:
> On top tagging with 10% labels:  
> **HEP-JEPA** achieves 86.9% accuracy and 0.94 AUROC — outperforming baselines with fewer parameters.

# What’s Next?

This is just the beginning. We're looking at:
- Adapting HEP-JEPA to anomaly detection and detector unfolding  
- Scaling to generative tasks  
- Improving physics-based priors in architecture

You can learn more at the [ICLR 2025 HEP-JEPA Workshop](https://hep-jepa.github.io/).

---

Thanks for reading! Questions or ideas? Feel free to [reach out](mailto:rhagrawal1.0.7@gmail.com) or check out the [paper on arXiv](https://arxiv.org/abs/2502.03933).

